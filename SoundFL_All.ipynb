{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c8ce32f0",
      "metadata": {},
      "source": [
        "# FedDalf Audio FL ( Federated Domain Adaptation & Lifelong Learning for Audio)\n",
        "Welcome to this Colab tutorial on federated learning using the FedDalf method!\n",
        "\n",
        "In this notebook, we will build a federated learning system using FedDalf and PyTorch. In Part 1, we will set up the model training pipeline and data loading with PyTorch. In Part 2, we will introduce FedDalf, a cutting-edge approach that integrates federated learning with domain adaptation and lifelong learning to enhance model performance across different domains.\n",
        "\n",
        "Explore FedDalf on GitHub ‚≠êÔ∏è to ask questions and get help.\n",
        "\n",
        "Let's get started! üöÄ\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86565458",
      "metadata": {},
      "source": [
        "## Pr√©paration (optionnel Colab) / Preparation (optional Colab)\n",
        "- Monter Drive (`drive.mount('/content/drive')`). / Mount Drive (`drive.mount('/content/drive')`).\n",
        "- Lancer `install_dependencies()` si l'environnement n'a pas les versions √©pingl√©es. / Run `install_dependencies()` if the environment lacks the pinned versions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66decb4f",
      "metadata": {},
      "source": [
        "### Installation / Installation\n",
        "Cette cellule installe les versions √©pingl√©es (TensorFlow/Flower/numpy/imgaug) pour √©viter les conflits en environnement Colab.\n",
        "This cell installs pinned versions (TensorFlow/Flower/numpy/imgaug) to avoid conflicts in Colab environments.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "984b5027",
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Installation optionnelle pour Colab / Optional installation for Colab ---\n",
        "import os\n",
        "\n",
        "def install_dependencies():\n",
        "    commands = [\n",
        "        \"pip uninstall -y cryptography numpy\",\n",
        "        \"pip install cryptography==44.0.3\",\n",
        "        \"pip install numpy==1.26.4\",\n",
        "        \"pip install -q flwr[simulation] tensorflow matplotlib smote_variants tfds-nightly scipy\",\n",
        "        \"pip install imgaug==0.4.0 --no-deps\",\n",
        "        \"pip install --force-reinstall numpy==1.26.4\",\n",
        "        \"pip install -U 'flwr[simulation]'\",\n",
        "    ]\n",
        "    for cmd in commands:\n",
        "        print(f\"Running: {cmd}\")\n",
        "        os.system(cmd)\n",
        "    try:\n",
        "        import flwr as _  # noqa: F401\n",
        "        import imgaug.augmenters as _  # noqa: F401\n",
        "        from cryptography.hazmat.bindings._rust import PKCS7UnpaddingContext  # noqa: F401\n",
        "    except ImportError:\n",
        "        raise SystemExit(\"Red√©marrer le runtime puis relancer cette cellule.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a71a0644",
      "metadata": {},
      "source": [
        "### Imports et configuration / Imports and configuration\n",
        "D√©finit les d√©pendances principales, les constantes globales (dimensions, classes, hyperparam√®tres) et les chemins d'export.\n",
        "Defines main dependencies, global constants (dimensions, classes, hyperparameters), and export paths.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "928388d1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Imports & configuration / Imports & configuration ---\n",
        "import re\n",
        "import json\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from flwr.common import Metrics\n",
        "import flwr as fl\n",
        "\n",
        "try:\n",
        "    import cv2\n",
        "except ImportError:\n",
        "    cv2 = None\n",
        "\n",
        "NUM_CLASSES = 17\n",
        "INPUT_DIM = (16, 8, 1)\n",
        "INDEXED_SLICES = [1, 3, 6, 8]\n",
        "BASE_LR = 1e-4\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 3\n",
        "NUM_ROUNDS = 50\n",
        "FRACTION_CLIENTS = 1.0\n",
        "MINIMUM_CLIENTS = 10\n",
        "INITIAL_PATH_ALL_USERS = \"/content/drive/MyDrive/FEDADL/history/\"\n",
        "INITIAL_PATH = os.path.join(INITIAL_PATH_ALL_USERS, \"evaluation/\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "adf50028",
      "metadata": {},
      "source": [
        "### Utilitaires d'E/S / IO utilities\n",
        "Fonctions pour cr√©er les dossiers, √©crire/relire les historiques et mettre √† jour les listes persistantes.\n",
        "Functions to create folders, write/read histories, and update persisted lists.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7424bf2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- IO utils / Outils d'E/S ---\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "def ensure_dir(path: str) -> None:\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "\n",
        "\n",
        "def saving_history_dict(history_dict: dict, path: str) -> None:\n",
        "    try:\n",
        "        with open(path, \"a\", encoding=\"utf-8\") as f:\n",
        "            f.write(str(history_dict))\n",
        "            f.write(\"\")\n",
        "        print(f\"History saved -> {path}\")\n",
        "    except OSError:\n",
        "        print(f\"Unable to write history to {path}\")\n",
        "\n",
        "\n",
        "def load_list_from_file(path: str, round_id: int) -> list:\n",
        "    if not os.path.exists(path):\n",
        "        return []\n",
        "    with open(path, encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line_dict = json.loads(re.sub(\"[']\", '\"', line))\n",
        "            values = list(line_dict.values())\n",
        "            if values and values[0] == round_id:\n",
        "                return values[1]\n",
        "    return []\n",
        "\n",
        "\n",
        "def update_list(filename: str, round_id: int, current: list) -> list:\n",
        "    if round_id == 0:\n",
        "        return current\n",
        "    last = load_list_from_file(filename, round_id)\n",
        "    if not last:\n",
        "        return current\n",
        "    return [c if c != -1 else l for c, l in zip(current, last)]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84ebc2f6",
      "metadata": {},
      "source": [
        "### Chargement et filtrage des donn√©es / Data loading and filtering\n",
        "Charge les paires features/labels .npy par client, effectue le split train/test et filtre les clients/contenus sur les classes cibl√©es.\n",
        "Loads per-client .npy feature/label pairs, performs train/test split, and filters clients/records on target classes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ac82492",
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Data loading & filtering / Chargement et filtrage ---\n",
        "def _load_numpy_pair(folder: str):\n",
        "    features, labels = None, None\n",
        "    for file in os.listdir(folder):\n",
        "        if not file.endswith(\".npy\"):\n",
        "            continue\n",
        "        path = os.path.join(folder, file)\n",
        "        if \"features\" in file:\n",
        "            features = np.load(path)\n",
        "        elif \"labels\" in file:\n",
        "            labels = np.load(path)\n",
        "    return features, labels\n",
        "\n",
        "\n",
        "def make_client_data(client_folders):\n",
        "    client_data = []\n",
        "    for folder in client_folders:\n",
        "        if not os.path.exists(folder):\n",
        "            continue\n",
        "        X, y = _load_numpy_pair(folder)\n",
        "        if X is None or y is None:\n",
        "            continue\n",
        "        y_cat = to_categorical(y, num_classes=NUM_CLASSES)\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y_cat, random_state=1)\n",
        "        X_train = X_train.reshape(len(X_train), *INPUT_DIM)\n",
        "        X_test = X_test.reshape(len(X_test), *INPUT_DIM)\n",
        "        client_data.append((X_train, y_train, X_test, y_test))\n",
        "    return client_data\n",
        "\n",
        "\n",
        "def resize_images(x_train, img_size):\n",
        "    if cv2 is None:\n",
        "        raise ImportError(\"cv2 not available; install opencv-python to use resize_images\")\n",
        "    return np.stack([cv2.resize(img, (img_size, img_size)) for img in x_train])\n",
        "\n",
        "\n",
        "def number_of_labels(y_train):\n",
        "    y_arr = np.asarray(y_train)\n",
        "    if y_arr.ndim > 1:\n",
        "        return int(np.sum(~np.isnan(y_arr[:, 0])))\n",
        "    return int(np.sum(~np.isnan(y_arr)))\n",
        "\n",
        "\n",
        "def renew_list(size: int = NUM_CLASSES):\n",
        "    return [0] * size\n",
        "\n",
        "\n",
        "def filter_clients_by_classes(all_x, all_y, indexed_slices):\n",
        "    selected_x, selected_y = [], []\n",
        "    for x_train, y_train in zip(all_x, all_y):\n",
        "        class_counts = renew_list()\n",
        "        for label in y_train:\n",
        "            class_counts[int(np.argmax(label))] += 1\n",
        "        if any(class_counts[i] == 0 for i in indexed_slices):\n",
        "            continue\n",
        "        mask = np.isin(np.argmax(y_train, axis=1), indexed_slices)\n",
        "        selected_x.append(x_train[mask])\n",
        "        selected_y.append(y_train[mask])\n",
        "    return selected_x, selected_y\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e5a12d2",
      "metadata": {},
      "source": [
        "### Gestion des labels / Label handling\n",
        "Outils pour simuler des labels manquants, g√©n√©rer/mettre √† jour des pseudo-labels et s√©parer jeux √©tiquet√©s vs non √©tiquet√©s.\n",
        "Tools to simulate missing labels, generate/update pseudo-labels, and split labeled vs unlabeled sets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "beb952f3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Label utilities / Gestion des labels ---\n",
        "def disturb_labels(y_train, n):\n",
        "    y_copy = np.asarray(y_train, dtype=float).copy()\n",
        "    if n <= 0:\n",
        "        return y_copy\n",
        "    cut = max(len(y_copy) - n, 0)\n",
        "    y_copy[cut:] = np.nan\n",
        "    return y_copy\n",
        "\n",
        "\n",
        "def generate_one_hotpot_vector(position, size):\n",
        "    vector = [0.0] * size\n",
        "    vector[position] = 1.0\n",
        "    return vector\n",
        "\n",
        "\n",
        "def map_predict(y_pred, threshold):\n",
        "    updated, count = [], 0\n",
        "    for row in y_pred:\n",
        "        pos = int(np.argmax(row))\n",
        "        acc = float(row[pos])\n",
        "        if acc >= threshold:\n",
        "            updated.append(generate_one_hotpot_vector(pos, len(row)))\n",
        "            count += 1\n",
        "        else:\n",
        "            updated.append(np.nan)\n",
        "    return np.array(updated, dtype=object), count\n",
        "\n",
        "\n",
        "def update_y_train(y_train, y_pred):\n",
        "    y_train = np.asarray(y_train)\n",
        "    y_pred = np.asarray(y_pred, dtype=object)\n",
        "    for idx, label in enumerate(y_train):\n",
        "        has_label = (not isinstance(label, float)) and (not np.isnan(label).any()) if hasattr(label, \"any\") else not np.isnan(label)\n",
        "        if has_label:\n",
        "            y_pred[idx] = label\n",
        "    return y_pred\n",
        "\n",
        "\n",
        "def get_labeled_set(x_train, y_train):\n",
        "    y_arr = np.asarray(y_train)\n",
        "    mask = ~np.isnan(y_arr).any(axis=1) if y_arr.ndim > 1 else ~np.isnan(y_arr)\n",
        "    return x_train[mask], y_arr[mask]\n",
        "\n",
        "\n",
        "def get_unlabeled_set(x_train, y_train):\n",
        "    y_arr = np.asarray(y_train)\n",
        "    mask = np.isnan(y_arr).any(axis=1) if y_arr.ndim > 1 else np.isnan(y_arr)\n",
        "    return x_train[mask], y_arr[mask]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65fd7c55",
      "metadata": {},
      "source": [
        "### Aides clients/cat√©gories / Client/category helpers\n",
        "Normalisation des listes par identifiant client et s√©lection des statuts/cat√©gories pour le suivi f√©d√©r√©.\n",
        "Normalize lists by client id and select statuses/categories for federated tracking.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8fe1b7be",
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Client/category helpers / Aides clients-cat√©gories ---\n",
        "def get_normalized_list(nb_total, clients_name, client_x):\n",
        "    normalized = [-1] * nb_total\n",
        "    for client, elt in zip(clients_name, client_x):\n",
        "        normalized[int(client)] = elt\n",
        "    return normalized\n",
        "\n",
        "\n",
        "def get_selected_categorie_set(nb_total, clients_name, clients_status, categorie_list):\n",
        "    normalized_status = get_normalized_list(nb_total, clients_name, clients_status)\n",
        "    return [1 if status in categorie_list else 0 for status in normalized_status]\n",
        "\n",
        "\n",
        "def create_dictionary(names_list, values_list):\n",
        "    return dict(zip(names_list, values_list))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1513710d",
      "metadata": {},
      "source": [
        "### Mod√®le CNN / CNN model\n",
        "D√©finit et compile le CNN Keras (tanh, 17 classes) utilis√© par tous les clients f√©d√©r√©s.\n",
        "Defines and compiles the Keras CNN (tanh, 17 classes) used by all federated clients.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7fccde5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Model definition / D√©finition du mod√®le ---\n",
        "def create_keras_model():\n",
        "    model = models.Sequential([\n",
        "        layers.Conv2D(64, (3, 3), padding=\"same\", activation=\"tanh\", input_shape=INPUT_DIM),\n",
        "        layers.MaxPool2D(pool_size=(2, 2)),\n",
        "        layers.Conv2D(128, (3, 3), padding=\"same\", activation=\"tanh\"),\n",
        "        layers.MaxPool2D(pool_size=(2, 2)),\n",
        "        layers.Dropout(0.1),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(1024, activation=\"tanh\"),\n",
        "        layers.Dense(NUM_CLASSES, activation=\"softmax\"),\n",
        "    ])\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=BASE_LR),\n",
        "                  loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31ad0f8f",
      "metadata": {},
      "source": [
        "### Client Flower et strat√©gie / Flower client and strategy\n",
        "Impl√©mente le client Flower (fit/evaluate), la strat√©gie FedAvg personnalis√©e et la configuration d'agr√©gation/logs.\n",
        "Implements the Flower client (fit/evaluate), customized FedAvg strategy, and aggregation/logging setup.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79849623",
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Flower client & strategy / Client Flower et strat√©gie ---\n",
        "class FlowerClient(fl.client.NumPyClient):\n",
        "    def __init__(self, model, train_x, train_y, val_x, val_y, cid):\n",
        "        self.model = model\n",
        "        self.train_x = train_x\n",
        "        self.train_y = train_y\n",
        "        self.val_x = val_x\n",
        "        self.val_y = val_y\n",
        "        self.cid = cid\n",
        "\n",
        "    def get_parameters(self, config):\n",
        "        return self.model.get_weights()\n",
        "\n",
        "    def fit(self, parameters, config):\n",
        "        self.model.set_weights(parameters)\n",
        "        history = self.model.fit(self.train_x, self.train_y, epochs=EPOCHS,\n",
        "                                 validation_data=(self.val_x, self.val_y), verbose=0)\n",
        "        current_round = config.get(\"current_round\", 0)\n",
        "        client_name = f\"client_{self.cid}\"\n",
        "        saving_history_dict({f\"round{current_round}\": history.history}, os.path.join(INITIAL_PATH, f\"{client_name}.txt\"))\n",
        "        loss, acc = self.model.evaluate(self.val_x, self.val_y, verbose=0)\n",
        "        saving_history_dict({f\"round{current_round}\": {\"Local_loss\": [loss], \"Local_accuracy\": [acc]}},\n",
        "                            os.path.join(INITIAL_PATH, f\"Local_{client_name}.txt\"))\n",
        "        return self.model.get_weights(), len(self.train_x), {\"cid\": self.cid}\n",
        "\n",
        "    def evaluate(self, parameters, config):\n",
        "        self.model.set_weights(parameters)\n",
        "        current_round = config.get(\"current_round\", 0)\n",
        "        loss, acc = self.model.evaluate(self.val_x, self.val_y, verbose=0)\n",
        "        client_name = f\"client_{self.cid}\"\n",
        "        saving_history_dict({f\"round{current_round}\": {\"Global_loss\": [loss], \"Global_accuracy\": [acc]}},\n",
        "                            os.path.join(INITIAL_PATH, f\"Eval_{client_name}.txt\"))\n",
        "        if current_round == NUM_ROUNDS:\n",
        "            try:\n",
        "                self.model.save(os.path.join(INITIAL_PATH, \"model\"))\n",
        "            except Exception:\n",
        "                pass\n",
        "        return float(loss), len(self.val_x), {\"cid\": self.cid, \"accuracy\": float(acc), \"loss\": float(loss), \"round\": current_round}\n",
        "\n",
        "\n",
        "def weighted_average(metrics):\n",
        "    accuracies = [num_examples * m[\"accuracy\"] for num_examples, m in metrics]\n",
        "    losses = [num_examples * m[\"loss\"] for num_examples, m in metrics]\n",
        "    examples = [num_examples for num_examples, _ in metrics]\n",
        "    global_accuracy = sum(accuracies) / sum(examples)\n",
        "    global_loss = sum(losses) / sum(examples)\n",
        "    current_round = metrics[0][1].get(\"round\", 0) if metrics else 0\n",
        "    saving_history_dict({f\"round{current_round}\": {\"eval_loss\": [global_loss], \"eval_accuracy\": [global_accuracy]}},\n",
        "                        os.path.join(INITIAL_PATH, \"Evaluation.txt\"))\n",
        "    return {\"accuracy\": global_accuracy}\n",
        "\n",
        "\n",
        "def fit_config(server_round: int):\n",
        "    return {\"batch_size\": BATCH_SIZE, \"current_round\": server_round, \"local_epochs\": EPOCHS}\n",
        "\n",
        "\n",
        "def eval_config(server_round: int):\n",
        "    return {\"current_round\": server_round}\n",
        "\n",
        "\n",
        "class SaveModelStrategy(fl.server.strategy.FedAvg):\n",
        "    def configure_fit(self, server_round, parameters, client_manager):\n",
        "        client_fit_ins_list = super().configure_fit(server_round, parameters, client_manager)\n",
        "        selected = [client.cid for client, _ in client_fit_ins_list]\n",
        "        clients_status = [1 if str(i) in selected else 0 for i in range(len(client_manager.all()))]\n",
        "        saving_history_dict(create_dictionary([\"round\", \"status\"], [server_round, clients_status]),\n",
        "                            os.path.join(INITIAL_PATH, \"selected.txt\"))\n",
        "        return client_fit_ins_list\n",
        "\n",
        "    def aggregate_fit(self, server_round, results, failures):\n",
        "        for _, parameters in results:\n",
        "            print(\"Client:\", parameters.metrics.get(\"cid\"))\n",
        "        return super().aggregate_fit(server_round, results, failures)\n",
        "\n",
        "\n",
        "def client_fn_builder(all_x, all_y):\n",
        "    def client_fn(cid: str):\n",
        "        idx = int(cid)\n",
        "        x_train, x_test, y_train, y_test = train_test_split(all_x[idx], all_y[idx], test_size=0.2, random_state=42)\n",
        "        model = create_keras_model()\n",
        "        return FlowerClient(model, x_train, y_train, x_test, y_test, cid).to_client()\n",
        "    return client_fn\n",
        "\n",
        "\n",
        "def start_federated_simulation(all_x, all_y):\n",
        "    ensure_dir(INITIAL_PATH)\n",
        "    num_clients = len(all_x) if all_x else MINIMUM_CLIENTS\n",
        "    strategy = SaveModelStrategy(\n",
        "        fraction_fit=FRACTION_CLIENTS,\n",
        "        fraction_evaluate=FRACTION_CLIENTS,\n",
        "        min_fit_clients=max(MINIMUM_CLIENTS, num_clients),\n",
        "        min_evaluate_clients=num_clients,\n",
        "        min_available_clients=max(MINIMUM_CLIENTS, num_clients),\n",
        "        on_fit_config_fn=fit_config,\n",
        "        on_evaluate_config_fn=eval_config,\n",
        "        evaluate_metrics_aggregation_fn=weighted_average,\n",
        "        initial_parameters=fl.common.ndarrays_to_parameters(create_keras_model().get_weights()),\n",
        "    )\n",
        "    fl.simulation.start_simulation(\n",
        "        client_fn=client_fn_builder(all_x, all_y),\n",
        "        num_clients=num_clients,\n",
        "        config=fl.server.ServerConfig(num_rounds=NUM_ROUNDS),\n",
        "        strategy=strategy,\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c7fea95",
      "metadata": {},
      "source": [
        "### Orchestration / Orchestration\n",
        "Pipeline principal : initialise les chemins, charge et filtre les donn√©es, affiche la distribution puis lance la simulation f√©d√©r√©e.\n",
        "Main pipeline: initialize paths, load/filter data, print distributions, then launch federated simulation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "263e8724",
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Orchestration / Orchestration ---\n",
        "def main():\n",
        "    ensure_dir(INITIAL_PATH)\n",
        "    client_folders = [\n",
        "        \"/content/drive/MyDrive/numpyDataset\",\n",
        "        \"/content/drive/MyDrive/urbansound8k\",\n",
        "    ]\n",
        "    data_all = make_client_data(client_folders)\n",
        "    all_X_train, all_y_train = [], []\n",
        "    for x_tr, y_tr, _, _ in data_all:\n",
        "        all_X_train.append(np.array(x_tr))\n",
        "        all_y_train.append(np.array(y_tr))\n",
        "    if not all_X_train:\n",
        "        print(\"‚ö†Ô∏è No data loaded. Check Google Drive paths.\")\n",
        "        return\n",
        "    new_all_x, new_all_y = filter_clients_by_classes(all_X_train, all_y_train, INDEXED_SLICES)\n",
        "    for idx, labels in enumerate(new_all_y):\n",
        "        counts = renew_list()\n",
        "        for lbl in labels:\n",
        "            counts[int(np.argmax(lbl))] += 1\n",
        "        print(f\"Client {idx} distribution: {counts} (total={sum(counts)})\")\n",
        "    start_federated_simulation(new_all_x, new_all_y)\n",
        "\n",
        "# main()  # Uncomment to launch directly\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
